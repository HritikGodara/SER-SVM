# -*- coding: utf-8 -*-
"""SER-SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_QXdUzoIhRc-aqmNfoiK82KWK-A7ellE

# Downloading Datasets
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("ejlok1/cremad")

print("Path to dataset files:", path)

import kagglehub

# Download latest version
path = kagglehub.dataset_download("uwrfkaggler/ravdess-emotional-speech-audio")

print("Path to dataset files:", path)

import kagglehub

# Download latest version
path = kagglehub.dataset_download("ejlok1/toronto-emotional-speech-set-tess")

print("Path to dataset files:", path)

import kagglehub

# Download latest version
path = kagglehub.dataset_download("ejlok1/surrey-audiovisual-expressed-emotion-savee")

print("Path to dataset files:", path)

"""# Day-1(26-01-2025)

## Importing Libraries
"""

import pandas as pd
import numpy as np

import os
import sys

# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.
import librosa
import librosa.display
import seaborn as sns
import matplotlib.pyplot as plt
import joblib

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

from PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QLabel, QFileDialog, QVBoxLayout

# to play the audio files
from IPython.display import Audio

import keras
from keras.callbacks import ReduceLROnPlateau
from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Dropout, Flatten, BatchNormalization
from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint

import tensorflow as tf

import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)

"""## Data preparation"""

#Paths for data
Ravdess = "/root/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1/audio_speech_actors_01-24/"
Crema = "/root/.cache/kagglehub/datasets/ejlok1/cremad/versions/1/AudioWAV/"
Tess = "/root/.cache/kagglehub/datasets/ejlok1/toronto-emotional-speech-set-tess/versions/1/TESS Toronto emotional speech set data/"
Savee = "/root/.cache/kagglehub/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee/versions/1/ALL/"

"""### 1.Ravdess Dataframe"""

ravdess_dir_list = os.listdir(Ravdess)

file_emotion = []
file_path = []
for dir in ravdess_dir_list:
    # as their are 20 different actors in our previous directory we need to extract files for each actor.
    # Use os.path.join to create the correct path
    actor_dir = os.path.join(Ravdess, dir)
    actor = os.listdir(actor_dir)

    for file in actor:
      part = file.split(".")[0]
      part = part.split("-")
      # third part in each file represents the emotion associated to that file.
      file_emotion.append(int(part[2]))
      # Use os.path.join to create the correct path for the file
      file_path.append(os.path.join(actor_dir, file))

      # dataframe for emotion of files
      emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

      # dataframe for path of files.
      path_df = pd.DataFrame(file_path, columns=['Path'])
      Ravdess_df = pd.concat([emotion_df, path_df], axis=1)

      # changing integers to actual emotions.
      Ravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)
      Ravdess_df.head()

"""### 2. Crema DataFrame"""

crema_dir_list = os.listdir(Crema)

file_emotion = []
file_path = []

for file in crema_dir_list:
  # storing file paths
  file_path.append(Crema + file)
  # storing file emotions
  part = file.split('_')
  if part[2] == 'SAD':
    file_emotion.append('sad')
  elif part[2] == 'ANG':
    file_emotion.append('angry')
  elif part[2] == 'DIS':
    file_emotion.append('disgust')
  elif part[2] == 'FEA':
    file_emotion.append('fear')
  elif part[2] == 'HAP':
    file_emotion.append('happy')
  elif part[2] == 'NEU':
    file_emotion.append('neutral')
  else:
    file_emotion.append('Unknown')

# dataframe for emotion of files
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

# dataframe for path of files.
path_df = pd.DataFrame(file_path, columns=['Path'])
Crema_df = pd.concat([emotion_df, path_df], axis=1)
Crema_df.head()

"""### 3. TESS dataframe"""

tess_dir_list = os.listdir(Tess)

file_emotion = []
file_path = []

for dir in tess_dir_list:
  # Use os.path.join to create the correct path to the subdirectory
  subdirectory_path = os.path.join(Tess, dir)
  directories = os.listdir(subdirectory_path)
  for file in directories:
    part = file.split('.')[0]
    part = part.split('_')[2]
    if part=='ps':
      file_emotion.append('surprise')
    else:
      file_emotion.append(part)
    # Use os.path.join to create the correct path to the file
    file_path.append(os.path.join(subdirectory_path, file))

# dataframe for emotion of files
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

# dataframe for path of files.
path_df = pd.DataFrame(file_path, columns=['Path'])
Tess_df = pd.concat([emotion_df, path_df], axis=1)
Tess_df.head()

"""4. SAVEE dataframe"""

savee_dir_list = os.listdir(Savee)

file_emotion = []
file_path = []

for file in savee_dir_list:
  file_path.append(Savee + file)
  part = file.split('_')[1]
  ele = part[:-6]
  if ele=='a':
    file_emotion.append('angry')
  elif ele=='d':
    file_emotion.append('disgust')
  elif ele=='f':
    file_emotion.append('fear')
  elif ele=='h':
    file_emotion.append('happy')
  elif ele=='n':
    file_emotion.append('neutral')
  elif ele=='sa':
    file_emotion.append('sad')
  else:
    file_emotion.append('surprise')

# dataframe for emotion of files
emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])

# dataframe for path of files.
path_df = pd.DataFrame(file_path, columns=['Path'])
savee_df = pd.concat([emotion_df, path_df], axis=1)
savee_df.head()

# creating Dataframe using all the 4 dataframes we created so far.
data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, savee_df], axis = 0)
data_path.to_csv("data_path.csv",index=False)
data_path.head()

"""# Day-2(30-01-2025)

## Data Visualisation and Exploration
"""

plt.title('Count of Emotions', size=16)
sns.countplot(data_path.Emotions)
plt.ylabel('Count', size=12)
plt.xlabel('Emotions', size=12)
sns.despine(top = True, right = True, left = False, bottom = False)
plt.show()

def create_waveplot(data, sr, e):
    plt.figure(figsize=(10, 3))
    plt.title(f'Waveplot for {e}', size=15)
    librosa.display.waveshow(data, sr=sr) # Changed waveplot to waveshow
    plt.show()

def create_spectrogram(data, sr, e):
    # stft function converts the data into short term fourier transform
    X = librosa.stft(data)
    Xdb = librosa.amplitude_to_db(abs(X))
    plt.figure(figsize=(12, 3))
    plt.title(f'Spectrogram for {e}', size=15)
    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')
    plt.colorbar()

emotion = 'fear'
path = np.array(data_path.Path[data_path.Emotions == emotion])[1]
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'angry'
path = np.array(data_path.Path[data_path.Emotions == emotion])[1]
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'sad'
path = np.array(data_path.Path[data_path.Emotions == emotion])[1]
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'happy'
path = np.array(data_path.Path[data_path.Emotions == emotion])[1]
data, sampling_rate = librosa.load(path)
create_waveplot(data, sampling_rate, emotion)
create_spectrogram(data, sampling_rate, emotion)
Audio(path)

"""## Data Augmentation"""

def noise(data):
  noise_amp = 0.035*np.random.uniform()*np.amax(data)
  data = data + noise_amp*np.random.normal(size=data.shape[0])
  return data

def stretch(data, rate=0.8):
  return librosa.effects.time_stretch(y=data, rate=rate)

def shift(data):
  shift_range = int(np.random.uniform(low=-5, high = 5)*1000)
  return np.roll(data, shift_range)

def pitch(data, sampling_rate, pitch_factor=0.7):
  return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)

path = np.array(data_path.Path)[1]
data, sample_rate = librosa.load(path)

"""### 1. Simple Audio"""

plt.figure(figsize=(14, 4))
librosa.display.waveshow(noise(data), sr=sample_rate)
Audio(data=noise(data), rate=sample_rate)
Audio(path)

"""### 2. Noise Injection"""

x = noise(data)
plt.figure(figsize = (14, 4))
librosa.display.waveshow(x, sr = sample_rate)
Audio(x, rate = sample_rate)

"""### 3. Stretching"""

x = stretch(data)
plt.figure(figsize = (14, 4))
librosa.display.waveshow(x, sr = sample_rate)
Audio(x, rate = sample_rate)

"""### 4. Shifting"""

x = shift(data)
plt.figure(figsize = (14, 4))
librosa.display.waveshow(x, sr = sample_rate)
Audio(x, rate = sample_rate)

"""### 5. Pitch"""

x = pitch(data, sample_rate)
plt.figure(figsize = (14, 4))
librosa.display.waveshow(x, sr = sample_rate)
Audio(x, rate = sample_rate)

"""# Day-3()

### Feature Extraction
"""

def extract_feat(data, sample_rate):
    result = np.array([])

    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)
    result = np.hstack((result, zcr))

    stft = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)
    result = np.hstack((result, chroma_stft))

    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mfcc))

    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)
    result = np.hstack((result, rms))

    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)
    result = np.hstack((result, mel))

    return result

def get_feat(path):
    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)
    result = []

    # Original features
    result.append(extract_feat(data, sample_rate))

    # Augmented - Adding Noise
    noise_data = noise(data)
    result.append(extract_feat(noise_data, sample_rate))

    # Augmented - Time Stretch + Pitch Shift
    stretched_data = stretch(data)
    pitch_data = pitch(stretched_data, sample_rate)
    result.append(extract_feat(pitch_data, sample_rate))

    return np.array(result)

X, Y = [], []
for path, emotion in zip(data_path.Path, data_path.Emotions):
    feature = get_feat(path)
    for ele in feature:
        X.append(ele)
        Y.append(emotion)

# Convert to DataFrame and save
Feat = pd.DataFrame(X)
Feat['labels'] = Y
Feat.to_csv('features.csv', index=False)

print(Feat.head())

# Load extracted features
df = pd.read_csv("features.csv")

# Split into features (X) and labels (Y)
X = df.iloc[:, :-1].values
Y = df['labels'].values

# Encode labels
encoder = LabelEncoder()
Y = encoder.fit_transform(Y)

# Split dataset into training and testing
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Normalize features (important for SVM)
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""### Modelling"""

# Train SVM Model
svm = SVC(kernel='rbf', probability=True, gamma='auto')
svm.fit(x_train, y_train)

# Evaluate Model
y_pred = svm.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=encoder.classes_))

"""### GUI"""

svm = joblib.load("svm.pkl")
scaler = joblib.load("scaler.pkl")
encoder = joblib.load("encoder.pkl")

class EmotionRecognizer(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()

    def initUI(self):
        self.setWindowTitle("Speech Emotion Recognition")
        self.setGeometry(100, 100, 400, 200)

        layout = QVBoxLayout()

        self.label = QLabel("Emotion: N/A", self)
        layout.addWidget(self.label)

        btn = QPushButton("Select Audio", self)
        btn.clicked.connect(self.predict_emotion)
        layout.addWidget(btn)

        self.setLayout(layout)

    def extract_features(self, file_path):
        data, sample_rate = librosa.load(file_path, duration=2.5, offset=0.6)
        mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40).T, axis=0)
        return mfcc.reshape(1, -1)

    def predict_emotion(self):
        file_path, _ = QFileDialog.getOpenFileName(self, "Open Audio File", "", "WAV Files (*.wav)")
        if file_path:
            features = self.extract_features(file_path)
            features_scaled = scaler.transform(features)
            prediction = svm.predict(features_scaled)
            emotion = encoder.inverse_transform(prediction)[0]
            self.label.setText(f"Emotion: {emotion}")

app = QApplication([])
window = EmotionRecognizer()
window.show()
app.exec_()